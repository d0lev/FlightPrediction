{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4c777a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf \n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b769b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    dataset = dataset.apply(lambda x: (x - x.min(axis = 0)) / (x.max(axis = 0) - x.min(axis = 0)))\n",
    "    dataset = dataset.loc[:,dataset.any()]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "364a6bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARR_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49529</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49530</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49531</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49532</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49533</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49534 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARR_DELAY\n",
       "0            0.0\n",
       "1            0.0\n",
       "2            0.0\n",
       "3            0.0\n",
       "4            0.0\n",
       "...          ...\n",
       "49529        1.0\n",
       "49530        0.0\n",
       "49531        0.0\n",
       "49532        1.0\n",
       "49533        0.0\n",
       "\n",
       "[49534 rows x 1 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"2018.csv\" , index_col = 0)\n",
    "dataset = normalize(dataset.iloc[:,:])\n",
    "# Splitting the dataset into train 70% and test 30%\n",
    "features = (dataset.shape[1] - 1)\n",
    "examples = dataset.shape[0]\n",
    "train_size = int (0.7 * len(dataset))\n",
    "(x_train , y_train) = (np.array(dataset.iloc[:train_size,:features]), np.array(dataset.iloc[:train_size,features:]))\n",
    "(x_test , y_test) = (np.array(dataset.iloc[train_size:,:features]), np.array(dataset.iloc[train_size:,features:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2cb784d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters \n",
    "learning_rate = 0.0001\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "batches = int (train_size / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2216d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0  Accuracy: 1.0 loss: 0.79813\n",
      "Iteration:  1  Accuracy: 1.0 loss: 0.79717565\n",
      "Iteration:  2  Accuracy: 1.0 loss: 0.76197696\n",
      "Iteration:  3  Accuracy: 1.0 loss: 0.8196602\n",
      "Iteration:  4  Accuracy: 1.0 loss: 0.7752081\n",
      "Iteration:  5  Accuracy: 1.0 loss: 0.7264916\n",
      "Iteration:  6  Accuracy: 1.0 loss: 0.78338933\n",
      "Iteration:  7  Accuracy: 1.0 loss: 0.7682505\n",
      "Iteration:  8  Accuracy: 1.0 loss: 0.8239026\n",
      "Iteration:  9  Accuracy: 1.0 loss: 0.8321539\n",
      "Iteration:  10  Accuracy: 1.0 loss: 0.7522864\n",
      "Iteration:  11  Accuracy: 1.0 loss: 0.77940905\n",
      "Iteration:  12  Accuracy: 1.0 loss: 0.74214953\n",
      "Iteration:  13  Accuracy: 1.0 loss: 0.7963604\n",
      "Iteration:  14  Accuracy: 1.0 loss: 0.763468\n",
      "Iteration:  15  Accuracy: 1.0 loss: 0.7678264\n",
      "Iteration:  16  Accuracy: 1.0 loss: 0.75756484\n",
      "Iteration:  17  Accuracy: 1.0 loss: 0.7606062\n",
      "Iteration:  18  Accuracy: 1.0 loss: 0.74288255\n",
      "Iteration:  19  Accuracy: 1.0 loss: 0.78145844\n",
      "Iteration:  20  Accuracy: 1.0 loss: 0.7726476\n",
      "Iteration:  21  Accuracy: 1.0 loss: 0.79329073\n",
      "Iteration:  22  Accuracy: 1.0 loss: 0.79215086\n",
      "Iteration:  23  Accuracy: 1.0 loss: 0.75311315\n",
      "Iteration:  24  Accuracy: 1.0 loss: 0.75717854\n",
      "Iteration:  25  Accuracy: 1.0 loss: 0.76757383\n",
      "Iteration:  26  Accuracy: 1.0 loss: 0.7769539\n",
      "Iteration:  27  Accuracy: 1.0 loss: 0.73941386\n",
      "Iteration:  28  Accuracy: 1.0 loss: 0.73910147\n",
      "Iteration:  29  Accuracy: 1.0 loss: 0.74227774\n",
      "Iteration:  30  Accuracy: 1.0 loss: 0.7531456\n",
      "Iteration:  31  Accuracy: 1.0 loss: 0.74068904\n",
      "Iteration:  32  Accuracy: 1.0 loss: 0.77278715\n",
      "Iteration:  33  Accuracy: 1.0 loss: 0.7722377\n",
      "Iteration:  34  Accuracy: 1.0 loss: 0.7270979\n",
      "Iteration:  35  Accuracy: 1.0 loss: 0.7888394\n",
      "Iteration:  36  Accuracy: 1.0 loss: 0.72632927\n",
      "Iteration:  37  Accuracy: 1.0 loss: 0.7810497\n",
      "Iteration:  38  Accuracy: 1.0 loss: 0.78551126\n",
      "Iteration:  39  Accuracy: 1.0 loss: 0.7590245\n",
      "Iteration:  40  Accuracy: 1.0 loss: 0.74927515\n",
      "Iteration:  41  Accuracy: 1.0 loss: 0.74031305\n",
      "Iteration:  42  Accuracy: 1.0 loss: 0.73522234\n",
      "Iteration:  43  Accuracy: 1.0 loss: 0.7629449\n",
      "Iteration:  44  Accuracy: 1.0 loss: 0.7689291\n",
      "Iteration:  45  Accuracy: 1.0 loss: 0.75264174\n",
      "Iteration:  46  Accuracy: 1.0 loss: 0.76906705\n",
      "Iteration:  47  Accuracy: 1.0 loss: 0.7571912\n",
      "Iteration:  48  Accuracy: 1.0 loss: 0.77611214\n",
      "Iteration:  49  Accuracy: 1.0 loss: 0.74730456\n",
      "Iteration:  50  Accuracy: 1.0 loss: 0.73249847\n",
      "Iteration:  51  Accuracy: 1.0 loss: 0.75048584\n",
      "Iteration:  52  Accuracy: 1.0 loss: 0.7606572\n",
      "Iteration:  53  Accuracy: 1.0 loss: 0.7751738\n",
      "Iteration:  54  Accuracy: 1.0 loss: 0.7320842\n",
      "Iteration:  55  Accuracy: 1.0 loss: 0.7373465\n",
      "Iteration:  56  Accuracy: 1.0 loss: 0.7373865\n",
      "Iteration:  57  Accuracy: 1.0 loss: 0.77996176\n",
      "Iteration:  58  Accuracy: 1.0 loss: 0.74943435\n",
      "Iteration:  59  Accuracy: 1.0 loss: 0.7365349\n",
      "Iteration:  60  Accuracy: 1.0 loss: 0.74294823\n",
      "Iteration:  61  Accuracy: 1.0 loss: 0.73829633\n",
      "Iteration:  62  Accuracy: 1.0 loss: 0.7707161\n",
      "Iteration:  63  Accuracy: 1.0 loss: 0.7328888\n",
      "Iteration:  64  Accuracy: 1.0 loss: 0.71442467\n",
      "Iteration:  65  Accuracy: 1.0 loss: 0.7505641\n",
      "Iteration:  66  Accuracy: 1.0 loss: 0.744767\n",
      "Iteration:  67  Accuracy: 1.0 loss: 0.72224885\n",
      "Iteration:  68  Accuracy: 1.0 loss: 0.74556047\n",
      "Iteration:  69  Accuracy: 1.0 loss: 0.7355011\n",
      "Iteration:  70  Accuracy: 1.0 loss: 0.74443847\n",
      "Iteration:  71  Accuracy: 1.0 loss: 0.74772763\n",
      "Iteration:  72  Accuracy: 1.0 loss: 0.75136125\n",
      "Iteration:  73  Accuracy: 1.0 loss: 0.7490401\n",
      "Iteration:  74  Accuracy: 1.0 loss: 0.74770963\n",
      "Iteration:  75  Accuracy: 1.0 loss: 0.7310675\n",
      "Iteration:  76  Accuracy: 1.0 loss: 0.73639023\n",
      "Iteration:  77  Accuracy: 1.0 loss: 0.7285532\n",
      "Iteration:  78  Accuracy: 1.0 loss: 0.74635047\n",
      "Iteration:  79  Accuracy: 1.0 loss: 0.75166667\n",
      "Iteration:  80  Accuracy: 1.0 loss: 0.7475134\n",
      "Iteration:  81  Accuracy: 1.0 loss: 0.74814177\n",
      "Iteration:  82  Accuracy: 1.0 loss: 0.7326896\n",
      "Iteration:  83  Accuracy: 1.0 loss: 0.7338609\n",
      "Iteration:  84  Accuracy: 1.0 loss: 0.74325424\n",
      "Iteration:  85  Accuracy: 1.0 loss: 0.7162483\n",
      "Iteration:  86  Accuracy: 1.0 loss: 0.7230969\n",
      "Iteration:  87  Accuracy: 1.0 loss: 0.7468633\n",
      "Iteration:  88  Accuracy: 1.0 loss: 0.73536676\n",
      "Iteration:  89  Accuracy: 1.0 loss: 0.73887056\n",
      "Iteration:  90  Accuracy: 1.0 loss: 0.74105585\n",
      "Iteration:  91  Accuracy: 1.0 loss: 0.72030014\n",
      "Iteration:  92  Accuracy: 1.0 loss: 0.74090296\n",
      "Iteration:  93  Accuracy: 1.0 loss: 0.7219909\n",
      "Iteration:  94  Accuracy: 1.0 loss: 0.7348061\n",
      "Iteration:  95  Accuracy: 1.0 loss: 0.73255354\n",
      "Iteration:  96  Accuracy: 1.0 loss: 0.73470515\n",
      "Iteration:  97  Accuracy: 1.0 loss: 0.74250394\n",
      "Iteration:  98  Accuracy: 1.0 loss: 0.75579107\n",
      "Iteration:  99  Accuracy: 1.0 loss: 0.7318679\n",
      "Iteration:  100  Accuracy: 1.0 loss: 0.7380838\n",
      "Iteration:  101  Accuracy: 1.0 loss: 0.7428358\n",
      "Iteration:  102  Accuracy: 1.0 loss: 0.72143435\n",
      "Iteration:  103  Accuracy: 1.0 loss: 0.72731215\n",
      "Iteration:  104  Accuracy: 1.0 loss: 0.7644449\n",
      "Iteration:  105  Accuracy: 1.0 loss: 0.7333372\n",
      "Iteration:  106  Accuracy: 1.0 loss: 0.71802765\n",
      "Iteration:  107  Accuracy: 1.0 loss: 0.73984224\n",
      "Iteration:  108  Accuracy: 1.0 loss: 0.733366\n",
      "Iteration:  109  Accuracy: 1.0 loss: 0.7379431\n",
      "Iteration:  110  Accuracy: 1.0 loss: 0.7203825\n",
      "Iteration:  111  Accuracy: 1.0 loss: 0.72975\n",
      "Iteration:  112  Accuracy: 1.0 loss: 0.7358737\n",
      "Iteration:  113  Accuracy: 1.0 loss: 0.7319565\n",
      "Iteration:  114  Accuracy: 1.0 loss: 0.72478575\n",
      "Iteration:  115  Accuracy: 1.0 loss: 0.7177697\n",
      "Iteration:  116  Accuracy: 1.0 loss: 0.7263762\n",
      "Iteration:  117  Accuracy: 1.0 loss: 0.72975993\n",
      "Iteration:  118  Accuracy: 1.0 loss: 0.7157056\n",
      "Iteration:  119  Accuracy: 1.0 loss: 0.73402345\n",
      "Iteration:  120  Accuracy: 1.0 loss: 0.7373962\n",
      "Iteration:  121  Accuracy: 1.0 loss: 0.7263113\n",
      "Iteration:  122  Accuracy: 1.0 loss: 0.7123021\n",
      "Iteration:  123  Accuracy: 1.0 loss: 0.7458068\n",
      "Iteration:  124  Accuracy: 1.0 loss: 0.74414337\n",
      "Iteration:  125  Accuracy: 1.0 loss: 0.7106759\n",
      "Iteration:  126  Accuracy: 1.0 loss: 0.71752924\n",
      "Iteration:  127  Accuracy: 1.0 loss: 0.73522305\n",
      "Iteration:  128  Accuracy: 1.0 loss: 0.7148925\n",
      "Iteration:  129  Accuracy: 1.0 loss: 0.74065554\n",
      "Iteration:  130  Accuracy: 1.0 loss: 0.7245965\n",
      "Iteration:  131  Accuracy: 1.0 loss: 0.72161776\n",
      "Iteration:  132  Accuracy: 1.0 loss: 0.724801\n",
      "Iteration:  133  Accuracy: 1.0 loss: 0.71248114\n",
      "Iteration:  134  Accuracy: 1.0 loss: 0.71820116\n",
      "Iteration:  135  Accuracy: 1.0 loss: 0.7284856\n",
      "Iteration:  136  Accuracy: 1.0 loss: 0.71424913\n",
      "Iteration:  137  Accuracy: 1.0 loss: 0.7179069\n",
      "Iteration:  138  Accuracy: 1.0 loss: 0.69916886\n",
      "Iteration:  139  Accuracy: 1.0 loss: 0.71130675\n",
      "Iteration:  140  Accuracy: 1.0 loss: 0.7334077\n",
      "Iteration:  141  Accuracy: 1.0 loss: 0.75230825\n",
      "Iteration:  142  Accuracy: 1.0 loss: 0.73418564\n",
      "Iteration:  143  Accuracy: 1.0 loss: 0.71089816\n",
      "Iteration:  144  Accuracy: 1.0 loss: 0.7297598\n",
      "Iteration:  145  Accuracy: 1.0 loss: 0.70763725\n",
      "Iteration:  146  Accuracy: 1.0 loss: 0.73018384\n",
      "Iteration:  147  Accuracy: 1.0 loss: 0.7305505\n",
      "Iteration:  148  Accuracy: 1.0 loss: 0.7348749\n",
      "Iteration:  149  Accuracy: 1.0 loss: 0.7075249\n",
      "Iteration:  150  Accuracy: 1.0 loss: 0.71733993\n",
      "Iteration:  151  Accuracy: 1.0 loss: 0.71615195\n",
      "Iteration:  152  Accuracy: 1.0 loss: 0.71963257\n",
      "Iteration:  153  Accuracy: 1.0 loss: 0.72100794\n",
      "Iteration:  154  Accuracy: 1.0 loss: 0.7441104\n",
      "Iteration:  155  Accuracy: 1.0 loss: 0.7104414\n",
      "Iteration:  156  Accuracy: 1.0 loss: 0.71446997\n",
      "Iteration:  157  Accuracy: 1.0 loss: 0.72656924\n",
      "Iteration:  158  Accuracy: 1.0 loss: 0.7390274\n",
      "Iteration:  159  Accuracy: 1.0 loss: 0.72617453\n",
      "Iteration:  160  Accuracy: 1.0 loss: 0.7247009\n",
      "Iteration:  161  Accuracy: 1.0 loss: 0.7111704\n",
      "Iteration:  162  Accuracy: 1.0 loss: 0.72483355\n",
      "Iteration:  163  Accuracy: 1.0 loss: 0.71258616\n",
      "Iteration:  164  Accuracy: 1.0 loss: 0.7121211\n",
      "Iteration:  165  Accuracy: 1.0 loss: 0.72704756\n",
      "Iteration:  166  Accuracy: 1.0 loss: 0.72024214\n",
      "Iteration:  167  Accuracy: 1.0 loss: 0.7335353\n",
      "Iteration:  168  Accuracy: 1.0 loss: 0.71489465\n",
      "Iteration:  169  Accuracy: 1.0 loss: 0.72840834\n",
      "Iteration:  170  Accuracy: 1.0 loss: 0.72420985\n",
      "Iteration:  171  Accuracy: 1.0 loss: 0.7318512\n",
      "Iteration:  172  Accuracy: 1.0 loss: 0.72499305\n",
      "Iteration:  173  Accuracy: 1.0 loss: 0.7144677\n",
      "Iteration:  174  Accuracy: 1.0 loss: 0.7177072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  175  Accuracy: 1.0 loss: 0.71378416\n",
      "Iteration:  176  Accuracy: 1.0 loss: 0.7174969\n",
      "Iteration:  177  Accuracy: 1.0 loss: 0.7182917\n",
      "Iteration:  178  Accuracy: 1.0 loss: 0.7352327\n",
      "Iteration:  179  Accuracy: 1.0 loss: 0.7223719\n",
      "Iteration:  180  Accuracy: 1.0 loss: 0.73090774\n",
      "Iteration:  181  Accuracy: 1.0 loss: 0.72331053\n",
      "Iteration:  182  Accuracy: 1.0 loss: 0.71120554\n",
      "Iteration:  183  Accuracy: 1.0 loss: 0.71159285\n",
      "Iteration:  184  Accuracy: 1.0 loss: 0.72258955\n",
      "Iteration:  185  Accuracy: 1.0 loss: 0.7268301\n",
      "Iteration:  186  Accuracy: 1.0 loss: 0.7372057\n",
      "Iteration:  187  Accuracy: 1.0 loss: 0.71687806\n",
      "Iteration:  188  Accuracy: 1.0 loss: 0.70677567\n",
      "Iteration:  189  Accuracy: 1.0 loss: 0.72669816\n",
      "Iteration:  190  Accuracy: 1.0 loss: 0.7091745\n",
      "Iteration:  191  Accuracy: 1.0 loss: 0.72755766\n",
      "Iteration:  192  Accuracy: 1.0 loss: 0.7047157\n",
      "Iteration:  193  Accuracy: 1.0 loss: 0.7122081\n",
      "Iteration:  194  Accuracy: 1.0 loss: 0.72565514\n",
      "Iteration:  195  Accuracy: 1.0 loss: 0.71476656\n",
      "Iteration:  196  Accuracy: 1.0 loss: 0.71887285\n",
      "Iteration:  197  Accuracy: 1.0 loss: 0.71516263\n",
      "Iteration:  198  Accuracy: 1.0 loss: 0.71133393\n",
      "Iteration:  199  Accuracy: 1.0 loss: 0.713167\n",
      "Iteration:  200  Accuracy: 1.0 loss: 0.7273134\n",
      "Iteration:  201  Accuracy: 1.0 loss: 0.70281404\n",
      "Iteration:  202  Accuracy: 1.0 loss: 0.7295665\n",
      "Iteration:  203  Accuracy: 1.0 loss: 0.72479904\n",
      "Iteration:  204  Accuracy: 1.0 loss: 0.6943379\n",
      "Iteration:  205  Accuracy: 1.0 loss: 0.7043327\n",
      "Iteration:  206  Accuracy: 1.0 loss: 0.7373121\n",
      "Iteration:  207  Accuracy: 1.0 loss: 0.7160832\n",
      "Iteration:  208  Accuracy: 1.0 loss: 0.7069533\n",
      "Iteration:  209  Accuracy: 1.0 loss: 0.70773065\n",
      "Iteration:  210  Accuracy: 1.0 loss: 0.7132422\n",
      "Iteration:  211  Accuracy: 1.0 loss: 0.7082786\n",
      "Iteration:  212  Accuracy: 1.0 loss: 0.7172908\n",
      "Iteration:  213  Accuracy: 1.0 loss: 0.7118747\n",
      "Iteration:  214  Accuracy: 1.0 loss: 0.7130091\n",
      "Iteration:  215  Accuracy: 1.0 loss: 0.7143343\n",
      "Iteration:  216  Accuracy: 1.0 loss: 0.7167112\n",
      "Iteration:  217  Accuracy: 1.0 loss: 0.71479917\n",
      "Iteration:  218  Accuracy: 1.0 loss: 0.7286424\n",
      "Iteration:  219  Accuracy: 1.0 loss: 0.7160533\n",
      "Iteration:  220  Accuracy: 1.0 loss: 0.7303928\n",
      "Iteration:  221  Accuracy: 1.0 loss: 0.72065794\n",
      "Iteration:  222  Accuracy: 1.0 loss: 0.7089097\n",
      "Iteration:  223  Accuracy: 1.0 loss: 0.714511\n",
      "Iteration:  224  Accuracy: 1.0 loss: 0.71204877\n",
      "Iteration:  225  Accuracy: 1.0 loss: 0.71298605\n",
      "Iteration:  226  Accuracy: 1.0 loss: 0.7250502\n",
      "Iteration:  227  Accuracy: 1.0 loss: 0.7139584\n",
      "Iteration:  228  Accuracy: 1.0 loss: 0.7092378\n",
      "Iteration:  229  Accuracy: 1.0 loss: 0.7249553\n",
      "Iteration:  230  Accuracy: 1.0 loss: 0.71140486\n",
      "Iteration:  231  Accuracy: 1.0 loss: 0.7162916\n",
      "Iteration:  232  Accuracy: 1.0 loss: 0.7043129\n",
      "Iteration:  233  Accuracy: 1.0 loss: 0.71284056\n",
      "Iteration:  234  Accuracy: 1.0 loss: 0.7089804\n",
      "Iteration:  235  Accuracy: 1.0 loss: 0.716432\n",
      "Iteration:  236  Accuracy: 1.0 loss: 0.7235782\n",
      "Iteration:  237  Accuracy: 1.0 loss: 0.718484\n",
      "Iteration:  238  Accuracy: 1.0 loss: 0.71876353\n",
      "Iteration:  239  Accuracy: 1.0 loss: 0.71421355\n",
      "Iteration:  240  Accuracy: 1.0 loss: 0.7210841\n",
      "Iteration:  241  Accuracy: 1.0 loss: 0.7008583\n",
      "Iteration:  242  Accuracy: 1.0 loss: 0.7069293\n",
      "Iteration:  243  Accuracy: 1.0 loss: 0.7134787\n",
      "Iteration:  244  Accuracy: 1.0 loss: 0.71277976\n",
      "Iteration:  245  Accuracy: 1.0 loss: 0.70979017\n",
      "Iteration:  246  Accuracy: 1.0 loss: 0.7241833\n",
      "Iteration:  247  Accuracy: 1.0 loss: 0.7193476\n",
      "Iteration:  248  Accuracy: 1.0 loss: 0.6969522\n",
      "Iteration:  249  Accuracy: 1.0 loss: 0.70010376\n",
      "Iteration:  250  Accuracy: 1.0 loss: 0.71427613\n",
      "Iteration:  251  Accuracy: 1.0 loss: 0.70735306\n",
      "Iteration:  252  Accuracy: 1.0 loss: 0.7167242\n",
      "Iteration:  253  Accuracy: 1.0 loss: 0.701494\n",
      "Iteration:  254  Accuracy: 1.0 loss: 0.6997707\n",
      "Iteration:  255  Accuracy: 1.0 loss: 0.71401715\n",
      "Iteration:  256  Accuracy: 1.0 loss: 0.70800203\n",
      "Iteration:  257  Accuracy: 1.0 loss: 0.7080397\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-ce01fedfc091>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" Accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"loss:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    971\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    972\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1161\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1163\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32 , [None , features])\n",
    "y = tf.placeholder(tf.float32 , [None , 1])\n",
    "\n",
    "W = tf.Variable(tf.zeros([features, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "pred = tf.nn.sigmoid(tf.matmul(x,W) + b)  \n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = pred))\n",
    "\n",
    "update = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Define accuracy - Percentage of predictions did we get right\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1),tf.argmax(y, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "(x_test , y_test) = (np.array(dataset.iloc[train_size:,:features]), np.array(dataset.iloc[train_size:,features:]))\n",
    "\n",
    "\n",
    "for epoch in range(500):     \n",
    "    for row in range(batches):        \n",
    "        batch_x = x_train [row * batch_size : (row + 1) * batch_size]\n",
    "        batch_y = y_train [row * batch_size : (row + 1) * batch_size]\n",
    "        session.run(update ,feed_dict = {x : batch_x , y : batch_y})\n",
    "        _, loss_val = session.run([update, loss],feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "    print(\"Iteration: \",epoch, \" Accuracy:\", session.run(accuracy, feed_dict={x: x_test, y: y_test}),\"loss:\",loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "30feb402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3439892858528191"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logistic_fun(z):\n",
    "    return 1/(1.0 + np.exp(-z))\n",
    "\n",
    "x = dataset.iloc[7:8,:-1]\n",
    "logistic_fun(np.matmul(np.array(x),session.run(W)) + session.run(b))[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
